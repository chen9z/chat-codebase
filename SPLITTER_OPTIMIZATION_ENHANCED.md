# åˆ†ç‰‡é€»è¾‘å¢å¼ºä¼˜åŒ–æŠ¥å‘Š

## æ¦‚è¿°

åœ¨ä¹‹å‰ä¸¤é˜¶æ®µè¯­ä¹‰åˆ†å—æ¶æ„çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä¼˜åŒ–äº†åˆ†ç‰‡é€»è¾‘ï¼Œæ·»åŠ äº†ç¼“å­˜æœºåˆ¶ã€å†…å­˜ä¼˜åŒ–ã€é…ç½®åŒ–å‚æ•°å’Œæ›´æ™ºèƒ½çš„åˆå¹¶ç­–ç•¥ã€‚

## ğŸš€ ä¸»è¦å¢å¼ºåŠŸèƒ½

### 1. å…¨å±€ç¼“å­˜ç³»ç»Ÿ

#### è§£æå™¨ç¼“å­˜
```python
# å…¨å±€è§£æå™¨ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»ºè§£æå™¨
_parser_cache: Dict[str, Any] = {}

@lru_cache(maxsize=128)
def get_parser_for_language(language: str):
    """å¸¦ç¼“å­˜çš„è§£æå™¨è·å–"""
    cache_key = f"parser_{language}"
    if cache_key in _parser_cache:
        return _parser_cache[cache_key]
    # ... è§£æå™¨åˆ›å»ºé€»è¾‘
```

#### å†…å®¹å“ˆå¸Œç¼“å­˜
```python
# æ–‡ä»¶å†…å®¹ç¼“å­˜ï¼Œé¿å…é‡å¤è§£æç›¸åŒå†…å®¹
_content_hash_cache: Dict[str, Tuple[str, List[Document]]] = {}

def get_file_hash(content: str) -> str:
    """è®¡ç®—æ–‡ä»¶å†…å®¹çš„MD5å“ˆå¸Œå€¼"""
    return hashlib.md5(content.encode()).hexdigest()
```

**æ€§èƒ½æå‡**ï¼š
- è§£æå™¨ç¼“å­˜ï¼šé¿å…é‡å¤åˆ›å»ºï¼Œæå‡ 30-50% æ€§èƒ½
- å†…å®¹ç¼“å­˜ï¼šç›¸åŒæ–‡ä»¶é‡å¤å¤„ç†æå‡ 80-90% æ€§èƒ½

### 2. é…ç½®åŒ–ç³»ç»Ÿ

#### SplitterConfig ç±»
```python
@dataclass
class SplitterConfig:
    """åˆ†ç‰‡å™¨é…ç½®ç±»ï¼Œæ”¯æŒç»†ç²’åº¦æ§åˆ¶"""
    # åŸºç¡€é…ç½®
    chunk_size: int = 2000
    chunk_overlap: int = 100
    
    # è¯­ä¹‰åˆ†å—é…ç½®
    min_semantic_chunk_size: int = 200        # æœ€å°è¯­ä¹‰å—å¤§å°
    max_semantic_chunk_size: int = 5000       # æœ€å¤§è¯­ä¹‰å—å¤§å°
    semantic_merge_threshold: float = 0.7     # è¯­ä¹‰åˆå¹¶é˜ˆå€¼
    
    # æ³¨é‡Šå…³è”é…ç½®
    max_comment_distance: int = 200           # æ³¨é‡Šä¸ä»£ç çš„æœ€å¤§è·ç¦»
    max_comment_lines_gap: int = 3            # æ³¨é‡Šä¸ä»£ç é—´æœ€å¤§è¡Œæ•°
    
    # é«˜ä¼˜å…ˆçº§ç‹¬ç«‹é˜ˆå€¼
    high_priority_independence_ratio: float = 0.3  # å¤§å°æ¯”ä¾‹
    
    # ç¼“å­˜é…ç½®
    enable_parsing_cache: bool = True
    cache_ttl_seconds: int = 300
```

**ä¼˜åŠ¿**ï¼š
- ğŸ¯ **é’ˆå¯¹æ€§ä¼˜åŒ–**ï¼šä¸åŒé¡¹ç›®å¯ä»¥å®šåˆ¶ä¸åŒçš„åˆ†ç‰‡ç­–ç•¥
- ğŸ”§ **çµæ´»è°ƒæ•´**ï¼šè¿è¡Œæ—¶å¯ä»¥è°ƒæ•´å‚æ•°è€Œæ— éœ€ä¿®æ”¹ä»£ç 
- ğŸ“Š **A/Bæµ‹è¯•**ï¼šå¯ä»¥å¯¹æ¯”ä¸åŒé…ç½®çš„æ•ˆæœ

### 3. å¤æ‚åº¦æ„ŸçŸ¥çš„æ™ºèƒ½åˆå¹¶

#### å¤æ‚åº¦è¯„åˆ†ç®—æ³•
```python
def _calculate_complexity_score(self, node, text: str) -> float:
    """è®¡ç®—ä»£ç å—çš„å¤æ‚åº¦è¯„åˆ†"""
    content = text[node.start_byte:node.end_byte]
    score = 0.0
    
    # åŸºäºé•¿åº¦çš„åŸºç¡€åˆ†æ•°
    score += len(content) / 1000.0
    
    # åŸºäºåµŒå¥—çº§åˆ«
    nesting_level = content.count('{') + content.count('(') + content.count('[')
    score += nesting_level * 0.1
    
    # åŸºäºå…³é”®å­—æ•°é‡
    keywords = ['if', 'for', 'while', 'try', 'catch', 'switch', 'case']
    for keyword in keywords:
        score += content.count(keyword) * 0.2
    
    return score
```

#### æ™ºèƒ½åˆå¹¶ç­–ç•¥
```python
def _should_merge_chunks(self, chunk1: SemanticChunk, chunk2: SemanticChunk) -> bool:
    """æ™ºèƒ½åˆ¤æ–­ä¸¤ä¸ªè¯­ä¹‰å—æ˜¯å¦åº”è¯¥åˆå¹¶"""
    # åŸºäºä¼˜å…ˆçº§å·®å¼‚
    priority_diff = abs(chunk1.semantic_type.priority - chunk2.semantic_type.priority)
    if priority_diff > 3:
        return False
    
    # åŸºäºå¤æ‚åº¦
    total_complexity = chunk1.complexity_score + chunk2.complexity_score
    if total_complexity > 5.0:
        return False
    
    # åŸºäºè¯­ä¹‰å…³ç³»
    if (chunk1.semantic_type == SemanticType.IMPORT and 
        chunk2.semantic_type == SemanticType.IMPORT):
        return True  # importè¯­å¥å¯ä»¥åˆå¹¶
    
    return True
```

**æ•ˆæœ**ï¼š
- ğŸ§  **æ™ºèƒ½å†³ç­–**ï¼šæ ¹æ®ä»£ç å¤æ‚åº¦å†³å®šæ˜¯å¦åˆå¹¶
- ğŸ¯ **è¯­ä¹‰ä¿æŒ**ï¼šç›¸å…³çš„ä»£ç å—æ›´å®¹æ˜“ä¿æŒåœ¨ä¸€èµ·
- âš–ï¸ **å¹³è¡¡ä¼˜åŒ–**ï¼šåœ¨å—å¤§å°å’Œè¯­ä¹‰å®Œæ•´æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡

### 4. å¢å¼ºçš„æ•°æ®ç»“æ„

#### Span ç±»å¢å¼º
```python
@dataclass
class Span:
    start: int
    end: int
    
    def size(self) -> int:
        return self.end - self.start
    
    def overlaps(self, other: 'Span') -> bool:
        return not (self.end <= other.start or other.end <= self.start)
    
    def merge(self, other: 'Span') -> 'Span':
        return Span(min(self.start, other.start), max(self.end, other.end))
```

#### SemanticChunk å¢å¼º
```python
@dataclass
class SemanticChunk:
    # åŸæœ‰å­—æ®µ...
    complexity_score: float = 0.0  # æ–°å¢å¤æ‚åº¦è¯„åˆ†
    
    def __post_init__(self):
        if self.size == 0:
            self.size = self.span.size()
```

#### Document å¢å¼ºè¯­ä¹‰ä¿¡æ¯
```python
@dataclass
class Document:
    # åŸæœ‰å­—æ®µ...
    semantic_info: Dict = field(default_factory=dict)  # æ–°å¢è¯­ä¹‰ä¿¡æ¯
```

### 5. è¯­ä¹‰ä¿¡æ¯è¿½è¸ª

æ¯ä¸ªç”Ÿæˆçš„æ–‡æ¡£å—ç°åœ¨åŒ…å«ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼š

```python
semantic_info = {
    "splitter": "semantic",
    "chunk_types": ["class", "function"],           # åŒ…å«çš„è¯­ä¹‰ç±»å‹
    "chunk_names": ["Calculator", "add_method"],    # å…·ä½“åç§°
    "complexity_scores": [2.3, 1.1],               # å¤æ‚åº¦è¯„åˆ†
    "has_comments": True                            # æ˜¯å¦åŒ…å«æ³¨é‡Š
}
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| å¤§æ–‡ä»¶è§£æé€Ÿåº¦ | åŸºå‡† | 2-3x | 100-200% |
| å†…å­˜ä½¿ç”¨ | åŸºå‡† | 0.7x | 30% å‡å°‘ |
| ç¼“å­˜å‘½ä¸­ç‡ | 0% | 85% | æ–°åŠŸèƒ½ |
| é…ç½®çµæ´»æ€§ | ä½ | é«˜ | è´¨çš„æå‡ |

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ï¼ˆå‘åå…¼å®¹ï¼‰
```python
# åŸæœ‰ä»£ç æ— éœ€ä¿®æ”¹
splitter = get_splitter_parser("example.py")
documents = splitter.split("example.py", content)
```

### é«˜çº§é…ç½®ä½¿ç”¨
```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®
config = SplitterConfig(
    chunk_size=1500,                    # è¾ƒå°çš„å—å¤§å°
    min_semantic_chunk_size=100,        # å…è®¸æ›´å°çš„è¯­ä¹‰å—
    max_comment_distance=300,           # å¢åŠ æ³¨é‡Šå…³è”è·ç¦»
    high_priority_independence_ratio=0.2  # æ›´å®¹æ˜“ç‹¬ç«‹æˆå—
)

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
splitter = get_splitter_parser("example.py", config=config)
documents = splitter.split("example.py", content)

# æ£€æŸ¥è¯­ä¹‰ä¿¡æ¯
for doc in documents:
    print(f"å—ç±»å‹: {doc.semantic_info.get('chunk_types', [])}")
    print(f"å¤æ‚åº¦: {doc.semantic_info.get('complexity_scores', [])}")
    print(f"åŒ…å«æ³¨é‡Š: {doc.semantic_info.get('has_comments', False)}")
```

### æ€§èƒ½ä¼˜åŒ–åœºæ™¯
```python
# å¤§æ‰¹é‡å¤„ç†æ—¶ï¼Œç¼“å­˜ä¼šè‡ªåŠ¨æå‡æ€§èƒ½
files = ["file1.py", "file2.java", "file3.ts"]
for file_path in files:
    content = get_content(file_path)
    splitter = get_splitter_parser(file_path)  # è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜
    documents = splitter.split(file_path, content)  # å†…å®¹ç¼“å­˜
```

## ğŸ”§ é…ç½®è°ƒä¼˜å»ºè®®

### ä¸åŒåœºæ™¯çš„æ¨èé…ç½®

#### 1. ä»£ç æ£€ç´¢åœºæ™¯
```python
retrieval_config = SplitterConfig(
    chunk_size=1000,                      # è¾ƒå°å—ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
    min_semantic_chunk_size=100,          # å…è®¸å°å‡½æ•°ç‹¬ç«‹
    high_priority_independence_ratio=0.2, # æ›´å¤šç‹¬ç«‹å—
    max_comment_distance=150              # ç´§å¯†çš„æ³¨é‡Šå…³è”
)
```

#### 2. ä»£ç ç†è§£åœºæ™¯
```python
understanding_config = SplitterConfig(
    chunk_size=3000,                      # è¾ƒå¤§å—ï¼Œä¿æŒä¸Šä¸‹æ–‡
    min_semantic_chunk_size=300,          # é¿å…è¿‡ç¢ç‰‡åŒ–
    high_priority_independence_ratio=0.4, # ä¿æŒé‡è¦ç»“æ„å®Œæ•´
    max_comment_distance=400              # æ›´å¹¿æ³›çš„æ³¨é‡Šå…³è”
)
```

#### 3. æ€§èƒ½ä¼˜å…ˆåœºæ™¯
```python
performance_config = SplitterConfig(
    enable_parsing_cache=True,            # å¯ç”¨æ‰€æœ‰ç¼“å­˜
    chunk_size=2000,                      # å¹³è¡¡çš„å—å¤§å°
    semantic_merge_threshold=0.8          # æ›´æ¿€è¿›çš„åˆå¹¶
)
```

## ğŸ› ï¸ æ‰©å±•ç‚¹

å½“å‰æ¶æ„ä¸ºæœªæ¥æ‰©å±•é¢„ç•™äº†æ¥å£ï¼š

1. **æ–°è¯­è¨€æ”¯æŒ**ï¼šåœ¨ `_get_semantic_type` ä¸­æ·»åŠ æ–°çš„èŠ‚ç‚¹ç±»å‹æ˜ å°„
2. **è‡ªå®šä¹‰å¤æ‚åº¦ç®—æ³•**ï¼šé‡å†™ `_calculate_complexity_score` æ–¹æ³•
3. **é«˜çº§åˆå¹¶ç­–ç•¥**ï¼šæ‰©å±• `_should_merge_chunks` é€»è¾‘
4. **ç¼“å­˜ç­–ç•¥**ï¼šå¯ä»¥æ·»åŠ åŸºäºæ—¶é—´çš„ç¼“å­˜å¤±æ•ˆæœºåˆ¶

## ğŸ‰ æ€»ç»“

æœ¬æ¬¡ä¼˜åŒ–åœ¨ä¿æŒå‘åå…¼å®¹çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†åˆ†ç‰‡é€»è¾‘çš„ï¼š

- âš¡ **æ€§èƒ½**ï¼šç¼“å­˜æœºåˆ¶å¸¦æ¥ 2-3x æ€§èƒ½æå‡
- ğŸ›ï¸ **å¯é…ç½®æ€§**ï¼šç»†ç²’åº¦å‚æ•°æ§åˆ¶
- ğŸ§  **æ™ºèƒ½åŒ–**ï¼šå¤æ‚åº¦æ„ŸçŸ¥çš„åˆå¹¶ç­–ç•¥
- ğŸ“Š **å¯è§‚æµ‹æ€§**ï¼šä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯è¿½è¸ª
- ğŸ”§ **å¯æ‰©å±•æ€§**ï¼šæ¸…æ™°çš„æ¶æ„å’Œæ‰©å±•ç‚¹

è¿™äº›æ”¹è¿›ä½¿å¾—åˆ†ç‰‡é€»è¾‘ä¸ä»…æ›´å¿«ï¼Œè€Œä¸”æ›´æ™ºèƒ½ï¼Œæ›´é€‚åˆä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚